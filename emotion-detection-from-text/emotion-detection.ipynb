{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data Processing\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\n\n# Modeling\nimport catboost as cb\n\n# Text Processing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('stopwords', download_dir='/kaggle/working/')\n# wordnet configuration\nimport subprocess\nnltk.download('wordnet', download_dir='/kaggle/working/')\ncommand = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\nsubprocess.run(command.split())\nnltk.data.path.append('/kaggle/working/')\n\n# BERT\nimport torch\nfrom transformers import BertModel\nfrom transformers import BertTokenizer\n\n# Miscellanous\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/emotion-detection-from-text/tweet_emotions.csv\", index_col='tweet_id')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Overview","metadata":{}},{"cell_type":"code","source":"print(f\"{df.shape[0]} rows\")\nprint(f\"{df.shape[1]} columns\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Processing","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lowercase the tweet content\ndef lowercasing(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df['content'] = df['content'].str.lower()\n    return df\n\n# tokenize the tweet content\ndef tokenizing(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df['tokenized_content'] = df['content'].apply(word_tokenize)\n    return df\n\n# remove the stopwords\ndef remove_stopwords(row):\n    wo_stopwords = [word for word in row if word not in stop_words]\n    return wo_stopwords\n\n# lemmatization\ndef lemmatizing(row):\n    result = []\n    for word in row:\n        result.append(lemmatizer.lemmatize(word))\n    return result\n\n# label encoding\ndef transform_label(row):\n    mapping = {\n        \"neutral\": 0,\n        \"worry\": 1,\n        \"happiness\": 2,\n        \"sadness\": 3,\n        \"love\": 4, \n        \"surprise\": 5,\n        \"fun\": 6,\n        \"relief\": 7,\n        \"hate\": 8,\n        \"empty\": 9,\n        \"enthusiasm\": 10,\n        \"boredom\": 11,\n        \"anger\": 12\n    }\n    return mapping[row]\n\n# transform the data\ndef transform(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df = lowercasing(df)\n    df = tokenizing(df)\n    df['wo_stopwords'] = df['tokenized_content'].apply(remove_stopwords)\n    df['lemmatized_content'] = df['wo_stopwords'].apply(lemmatizing)\n    df['sentiment'] = df['sentiment'].apply(transform_label)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = transform(df)\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Embeddings","metadata":{}},{"cell_type":"markdown","source":"## Tf Idf","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=5000)\n\n# list to string\ndf['lemmatized_content'] = df['lemmatized_content'].apply(lambda x: ' '.join(x))\n\n# vectorizing\ntfidf_matrix = vectorizer.fit_transform(df['lemmatized_content'].values).toarray()\ntfidf_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT","metadata":{}},{"cell_type":"code","source":"def add_special_tokens(row):\n    return \"[CLS] \"+row+\" [SEP]\"\n\ndf['bert_content'] = df['content'].apply(add_special_tokens)\ndf.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pretrained model/tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bert_word_embeddings(text, tokenizer, model):\n    # Tokenize the input text\n    tokenized_text_detail = tokenizer(text, return_tensors='pt')\n    # Extract token IDs, token types, and attention mask\n    token_ids = tokenized_text_detail[\"input_ids\"]\n    token_masks = tokenized_text_detail[\"attention_mask\"]\n    # Run the text through the BERT model\n    outputs = model(input_ids=token_ids, attention_mask=token_masks)\n    # Get the hidden states\n    hidden_states = outputs[2]\n    # Stack the hidden states\n    stacked_hidden_states = torch.stack(hidden_states)\n    # Permute the dimensions\n    token_embeddings = stacked_hidden_states.permute(1, 2, 0, 3)\n    # Sum the last 4 layers for each token\n    token_vecs_sum = torch.sum(token_embeddings[-4:], dim=0)\n    return token_vecs_sum\n\n# Example usage:\ntext = df['bert_content'].loc[1956967341]\nembeddings = get_bert_word_embeddings(text, tokenizer, model)\nprint(embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embeddings_list = []\n\n# for text in df['bert_content']:\n#     embeddings = get_bert_word_embeddings(text, tokenizer, model)\n#     embeddings_list.append(embeddings)\n    \n# embeddings_array_list = [embedding.numpy() for embedding in embeddings_list]\n# embeddings_df = pd.DataFrame(embeddings_array_list)\n# embeddings_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\nI use catboost with different approach of word embeddings","metadata":{}},{"cell_type":"code","source":"def plot_training_loss(model):\n    # plot training loss\n    train_error = []\n    for score in model.evals_result_['learn']['MultiClass']:\n        train_error.append(score)\n    # Plot the training and validation error during grid search\n    plt.plot(train_error, label='Training Error')\n    plt.xlabel('Iteration')\n    plt.ylabel('MultiClass')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tf Idf","metadata":{}},{"cell_type":"code","source":"X_tfidf_train, X_tfidf_val, y_tfidf_train, y_tfidf_val = train_test_split(\n    tfidf_matrix, \n    df['sentiment'], \n    test_size=0.2, \n    random_state=12, \n    stratify=df['sentiment']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cbc = cb.CatBoostClassifier(verbose=50, loss_function='MultiClass')\ncbc.fit(X_tfidf_train, y_tfidf_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_loss(cbc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Catboost Text Features","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(\n    df[['content']], \n    df['sentiment'], \n    test_size=0.2, \n    random_state=12, \n    stratify=df['sentiment']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cbc2 = cb.CatBoostClassifier(verbose=50, loss_function='MultiClass', text_features=[\"content\"])\ncbc2.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_loss(cbc2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT","metadata":{}},{"cell_type":"code","source":"X_bert_train, X_bert_val, y_bert_train, y_bert_val = train_test_split(\n    embeddings_df, \n    df['sentiment'], \n    test_size=0.2, \n    random_state=12\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cbc3 = cb.CatBoostClassifier(verbose=50, loss_function='MultiClass', text_features=[\"content\"])\ncbc3.fit(X_bert_train, y_bert_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_loss(cbc3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"def evaluation(model, X_val, y_val):\n    y_pred = model.predict(X_val)\n    print(f\"Classification Report:\\n {classification_report(y_val, y_pred)}\\n\\n\")\n    cm = confusion_matrix(y_val, y_pred, labels=model.classes_)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n    disp.plot()\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF IDF\nevaluation(cbc, X_tfidf_val, y_tfidf_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# catboost with text features\nevaluation(cbc2, X_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}